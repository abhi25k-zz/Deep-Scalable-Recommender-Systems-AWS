{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-09 21:51:13--  http://files.grouplens.org/datasets/movielens/ml-10m.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.34.235\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.34.235|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 65566137 (63M) [application/zip]\n",
      "Saving to: ‘ml-10m.zip’\n",
      "\n",
      "ml-10m.zip          100%[===================>]  62.53M  50.8MB/s    in 1.2s    \n",
      "\n",
      "2018-12-09 21:51:15 (50.8 MB/s) - ‘ml-10m.zip’ saved [65566137/65566137]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-10m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-10m.zip\n",
      "   creating: ml-10M100K/\n",
      "  inflating: ml-10M100K/allbut.pl    \n",
      "  inflating: ml-10M100K/movies.dat   \n",
      "  inflating: ml-10M100K/ratings.dat  \n",
      "  inflating: ml-10M100K/README.html  \n",
      "  inflating: ml-10M100K/split_ratings.sh  \n",
      "  inflating: ml-10M100K/tags.dat     \n"
     ]
    }
   ],
   "source": [
    "!unzip ml-10m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "# Importing tensorflow\n",
    "import tensorflow as tf\n",
    "# Importing some more libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# reading the ratings data\n",
    "ratings = pd.read_csv('ml-10M100K/ratings.dat',\\\n",
    "          sep=\"::\", header = None, engine='python')\n",
    "# pivot the data to get it at a user level\n",
    "ratings_pivot = ratings[[0,1,2]].pivot(values=2, index=0, columns=1 ).fillna(0)\n",
    "# creating train and test sets\n",
    "X_train, X_test = train_test_split(ratings_pivot, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55902, 10677)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13976, 10677)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deciding how many nodes each layer should have\n",
    "n_nodes_inpl = 10677  \n",
    "n_nodes_hl1  = 256  \n",
    "n_nodes_outl = 10677  \n",
    "# first hidden layer has 784*32 weights and 32 biases\n",
    "hidden_1_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_inpl+1,n_nodes_hl1]))}\n",
    "output_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1+1,n_nodes_outl])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user with 3706 ratings goes in\n",
    "input_layer = tf.placeholder('float', [None, 10677])\n",
    "# add a constant node to the first layer\n",
    "# it needs to have the same shape as the input layer for me to be\n",
    "# able to concatinate it later\n",
    "input_layer_const = tf.fill( [tf.shape(input_layer)[0], 1] ,1.0  )\n",
    "input_layer_concat =  tf.concat([input_layer, input_layer_const], 1)\n",
    "# multiply output of input_layer wth a weight matrix \n",
    "layer_1 = tf.nn.sigmoid(tf.matmul(input_layer_concat,\\\n",
    "hidden_1_layer_vals['weights']))\n",
    "# adding one bias node to the hidden layer\n",
    "layer1_const = tf.fill( [tf.shape(layer_1)[0], 1] ,1.0  )\n",
    "layer_concat =  tf.concat([layer_1, layer1_const], 1)\n",
    "# multiply output of hidden with a weight matrix to get final output\n",
    "output_layer = tf.matmul( layer_concat,output_layer_vals['weights'])\n",
    "# output_true shall have the original shape for error calculations\n",
    "output_true = tf.placeholder('float', [None, 10677])\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "# define our optimizer\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising variables and starting the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# defining batch size, number of epochs and learning rate\n",
    "batch_size = 256  # how many images to use together for training\n",
    "hm_epochs =10    # how many times to go through the entire dataset\n",
    "tot_images = X_train.shape[0] # total number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the model for a 10 epochs taking 256 users in batches\n",
    "# total improvement is printed out after each epoch\n",
    "def execute():\n",
    "    for epoch in range(hm_epochs):\n",
    "        epoch_loss = 0    # initializing error as 0\n",
    "\n",
    "        for i in range(int(tot_images/batch_size)):\n",
    "            epoch_x = X_train[ i*batch_size : (i+1)*batch_size ]\n",
    "            _, c = sess.run([optimizer, meansq],\\\n",
    "                   feed_dict={input_layer: epoch_x, \\\n",
    "                   output_true: epoch_x})\n",
    "            epoch_loss += c\n",
    "\n",
    "        output_train = sess.run(output_layer,\\\n",
    "                   feed_dict={input_layer:X_train})\n",
    "        output_test = sess.run(output_layer,\\\n",
    "                       feed_dict={input_layer:X_test})\n",
    "\n",
    "        print('MSE train', MSE(output_train, X_train),'MSE test', MSE(output_test, X_test))      \n",
    "        print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 21.02667199444175 MSE test 21.269128203290155\n",
      "Epoch 0 / 10 loss: 10158.817810058594\n",
      "MSE train 12.540689016754229 MSE test 12.79793284500949\n",
      "Epoch 1 / 10 loss: 3471.212664604187\n",
      "MSE train 9.449818040178554 MSE test 9.690455625060112\n",
      "Epoch 2 / 10 loss: 2364.2586345672607\n",
      "MSE train 7.78168182759592 MSE test 8.007625569608386\n",
      "Epoch 3 / 10 loss: 1869.1128058433533\n",
      "MSE train 6.71481371102814 MSE test 6.928391925030984\n",
      "Epoch 4 / 10 loss: 1578.2136611938477\n",
      "MSE train 5.957622213273727 MSE test 6.162226510151415\n",
      "Epoch 5 / 10 loss: 1381.688039779663\n",
      "MSE train 5.386035596716523 MSE test 5.589230112988447\n",
      "Epoch 6 / 10 loss: 1237.6742644309998\n",
      "MSE train 4.937154088723258 MSE test 5.141065937009644\n",
      "Epoch 7 / 10 loss: 1126.9541451931\n",
      "MSE train 4.57182836938838 MSE test 4.774583705218716\n",
      "Epoch 8 / 10 loss: 1038.2290863990784\n",
      "MSE train 4.268671236691935 MSE test 4.4693376797802395\n",
      "Epoch 9 / 10 loss: 965.2999305725098\n"
     ]
    }
   ],
   "source": [
    "execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 21.013082131370552 MSE test 20.696800817140645\n",
      "Epoch 0 / 10 loss: 10134.440954208374\n",
      "MSE train 12.545776197496407 MSE test 12.337113599790111\n",
      "Epoch 1 / 10 loss: 3480.995397567749\n",
      "MSE train 9.447356102936848 MSE test 9.289391673178086\n",
      "Epoch 2 / 10 loss: 2365.3183670043945\n",
      "MSE train 7.775859909808188 MSE test 7.654208346241174\n",
      "Epoch 3 / 10 loss: 1869.3435215950012\n",
      "MSE train 6.702062047534048 MSE test 6.6122516655822\n",
      "Epoch 4 / 10 loss: 1576.1149287223816\n",
      "MSE train 5.946604326796058 MSE test 5.872933607783672\n",
      "Epoch 5 / 10 loss: 1379.101336479187\n",
      "MSE train 5.383724419242147 MSE test 5.31846624201368\n",
      "Epoch 6 / 10 loss: 1236.4591608047485\n",
      "MSE train 4.940174314430072 MSE test 4.882950850873307\n",
      "Epoch 7 / 10 loss: 1127.0782809257507\n",
      "MSE train 4.580763300649352 MSE test 4.528395800531564\n",
      "Epoch 8 / 10 loss: 1039.6132636070251\n",
      "MSE train 4.281752188552861 MSE test 4.232313117138328\n",
      "Epoch 9 / 10 loss: 967.9392924308777\n",
      "MSE train 4.026561890442507 MSE test 3.9807074758941194\n",
      "Epoch 0 / 10 loss: 907.4546358585358\n",
      "MSE train 3.807351140801348 MSE test 3.764142400981326\n",
      "Epoch 1 / 10 loss: 855.6300075054169\n",
      "MSE train 3.6157414967302506 MSE test 3.5751506877100794\n",
      "Epoch 2 / 10 loss: 810.7944602966309\n",
      "MSE train 3.4449895242537707 MSE test 3.408764069194174\n",
      "Epoch 3 / 10 loss: 771.2279431819916\n",
      "MSE train 3.2922645736917633 MSE test 3.2622425978513876\n",
      "Epoch 4 / 10 loss: 735.8571319580078\n",
      "MSE train 3.154840866592687 MSE test 3.1312698000802626\n",
      "Epoch 5 / 10 loss: 704.1508507728577\n",
      "MSE train 3.030472978474404 MSE test 3.0123943966165627\n",
      "Epoch 6 / 10 loss: 675.5181515216827\n",
      "MSE train 2.9171585804324787 MSE test 2.9033166685381206\n",
      "Epoch 7 / 10 loss: 649.5544322729111\n",
      "MSE train 2.8132595815969035 MSE test 2.8029494660404\n",
      "Epoch 8 / 10 loss: 625.8127247095108\n",
      "MSE train 2.7178296888328006 MSE test 2.7105494392945557\n",
      "Epoch 9 / 10 loss: 604.0162453651428\n",
      "MSE train 2.629791272327025 MSE test 2.625433724445963\n",
      "Epoch 0 / 10 loss: 583.9696055650711\n",
      "MSE train 2.5480086376793176 MSE test 2.5462121804952713\n",
      "Epoch 1 / 10 loss: 565.4073984622955\n",
      "MSE train 2.4721425376040833 MSE test 2.4721532485594007\n",
      "Epoch 2 / 10 loss: 548.1584606170654\n",
      "MSE train 2.4014364173968747 MSE test 2.403021592114352\n",
      "Epoch 3 / 10 loss: 532.1275290250778\n",
      "MSE train 2.3353695763111624 MSE test 2.338504931366782\n",
      "Epoch 4 / 10 loss: 517.1654313802719\n",
      "MSE train 2.2733874107164325 MSE test 2.278277567322313\n",
      "Epoch 5 / 10 loss: 503.1567749977112\n",
      "MSE train 2.215228284081527 MSE test 2.221880901538197\n",
      "Epoch 6 / 10 loss: 490.01505994796753\n",
      "MSE train 2.16048935044674 MSE test 2.1687829918143167\n",
      "Epoch 7 / 10 loss: 477.6751730442047\n",
      "MSE train 2.108493487184539 MSE test 2.1184919655174927\n",
      "Epoch 8 / 10 loss: 465.99849784374237\n",
      "MSE train 2.0590633531841713 MSE test 2.0708034571800686\n",
      "Epoch 9 / 10 loss: 454.92647540569305\n",
      "MSE train 2.012018181386644 MSE test 2.0254009619880367\n",
      "Epoch 0 / 10 loss: 444.38443315029144\n",
      "MSE train 1.9672126380190367 MSE test 1.982077491215268\n",
      "Epoch 1 / 10 loss: 434.3387701511383\n",
      "MSE train 1.9243241463983738 MSE test 1.9407343353657374\n",
      "Epoch 2 / 10 loss: 424.7576650381088\n",
      "MSE train 1.8832348155351475 MSE test 1.9012472304684893\n",
      "Epoch 3 / 10 loss: 415.5789645910263\n",
      "MSE train 1.8438781901886037 MSE test 1.8635275636429967\n",
      "Epoch 4 / 10 loss: 406.7927725315094\n",
      "MSE train 1.8061821871536607 MSE test 1.827465290466513\n",
      "Epoch 5 / 10 loss: 398.36635196208954\n",
      "MSE train 1.7699261276828024 MSE test 1.7928881170614075\n",
      "Epoch 6 / 10 loss: 390.2929536104202\n",
      "MSE train 1.735110495360705 MSE test 1.7597261708605967\n",
      "Epoch 7 / 10 loss: 382.52408266067505\n",
      "MSE train 1.7017550288741319 MSE test 1.7279825211215352\n",
      "Epoch 8 / 10 loss: 375.0714001059532\n",
      "MSE train 1.6697295608094411 MSE test 1.6975541495415767\n",
      "Epoch 9 / 10 loss: 367.9255191683769\n",
      "MSE train 1.6389547704156184 MSE test 1.668315723324867\n",
      "Epoch 0 / 10 loss: 361.0610240101814\n",
      "MSE train 1.6093538052073704 MSE test 1.640192022779947\n",
      "Epoch 1 / 10 loss: 354.45919239521027\n",
      "MSE train 1.5808845918715153 MSE test 1.6131203386233604\n",
      "Epoch 2 / 10 loss: 348.1190093755722\n",
      "MSE train 1.5534701870165104 MSE test 1.5870378595929107\n",
      "Epoch 3 / 10 loss: 342.0064646601677\n",
      "MSE train 1.526983916429437 MSE test 1.5618768973678756\n",
      "Epoch 4 / 10 loss: 336.119706094265\n",
      "MSE train 1.5014030029128869 MSE test 1.5376165184942538\n",
      "Epoch 5 / 10 loss: 330.4382234811783\n",
      "MSE train 1.4766801096120328 MSE test 1.5141824875422776\n",
      "Epoch 6 / 10 loss: 324.93786627054214\n",
      "MSE train 1.4528124600851122 MSE test 1.4915444518358716\n",
      "Epoch 7 / 10 loss: 319.61992704868317\n",
      "MSE train 1.4297646056081172 MSE test 1.4696614041056988\n",
      "Epoch 8 / 10 loss: 314.49086904525757\n",
      "MSE train 1.407518546376926 MSE test 1.4485163624303288\n",
      "Epoch 9 / 10 loss: 309.546121776104\n",
      "MSE train 1.3859979261683284 MSE test 1.4280637308212405\n",
      "Epoch 0 / 10 loss: 304.7605977654457\n",
      "MSE train 1.3651790888641808 MSE test 1.408263754356333\n",
      "Epoch 1 / 10 loss: 300.1276834011078\n",
      "MSE train 1.345054203750328 MSE test 1.389100126710276\n",
      "Epoch 2 / 10 loss: 295.6549786925316\n",
      "MSE train 1.3255434159621027 MSE test 1.3705426522810151\n",
      "Epoch 3 / 10 loss: 291.3243645429611\n",
      "MSE train 1.3066207444491944 MSE test 1.3525578122548931\n",
      "Epoch 4 / 10 loss: 287.12269389629364\n",
      "MSE train 1.288305209033992 MSE test 1.3351441025418223\n",
      "Epoch 5 / 10 loss: 283.05197209119797\n",
      "MSE train 1.270593239180143 MSE test 1.3182632443062434\n",
      "Epoch 6 / 10 loss: 279.1061730980873\n",
      "MSE train 1.2534438215694494 MSE test 1.301902517943168\n",
      "Epoch 7 / 10 loss: 275.297572016716\n",
      "MSE train 1.2367867337615193 MSE test 1.2860231984667916\n",
      "Epoch 8 / 10 loss: 271.60478979349136\n",
      "MSE train 1.22057481251222 MSE test 1.2705946232117977\n",
      "Epoch 9 / 10 loss: 268.0234714746475\n",
      "MSE train 1.204820438237966 MSE test 1.2556105439229046\n",
      "Epoch 0 / 10 loss: 264.5316908955574\n",
      "MSE train 1.1894639536931244 MSE test 1.2410471995919938\n",
      "Epoch 1 / 10 loss: 261.1288650035858\n",
      "MSE train 1.1745047876796622 MSE test 1.2268923696261171\n",
      "Epoch 2 / 10 loss: 257.82044917345047\n",
      "MSE train 1.159962420380761 MSE test 1.213146893871442\n",
      "Epoch 3 / 10 loss: 254.59524244070053\n",
      "MSE train 1.1458552847254404 MSE test 1.1997994552157043\n",
      "Epoch 4 / 10 loss: 251.45523810386658\n",
      "MSE train 1.1321651192425108 MSE test 1.186836079053426\n",
      "Epoch 5 / 10 loss: 248.4173246026039\n",
      "MSE train 1.1188741551754007 MSE test 1.1742449964491806\n",
      "Epoch 6 / 10 loss: 245.46992880105972\n",
      "MSE train 1.1059700803508137 MSE test 1.1620123935788986\n",
      "Epoch 7 / 10 loss: 242.61048686504364\n",
      "MSE train 1.0934298202972887 MSE test 1.1501205013442375\n",
      "Epoch 8 / 10 loss: 239.83127361536026\n",
      "MSE train 1.0812209542675502 MSE test 1.1385536101239264\n",
      "Epoch 9 / 10 loss: 237.1255978345871\n",
      "MSE train 1.0693522741996035 MSE test 1.1273008134597242\n",
      "Epoch 0 / 10 loss: 234.4894044995308\n",
      "MSE train 1.057821696707374 MSE test 1.1163519262328347\n",
      "Epoch 1 / 10 loss: 231.93208867311478\n",
      "MSE train 1.046605511959823 MSE test 1.105692197551277\n",
      "Epoch 2 / 10 loss: 229.44171607494354\n",
      "MSE train 1.0356644273822948 MSE test 1.0953033868253506\n",
      "Epoch 3 / 10 loss: 227.02221924066544\n",
      "MSE train 1.024997309245904 MSE test 1.0851818279272953\n",
      "Epoch 4 / 10 loss: 224.66531574726105\n",
      "MSE train 1.0145889569397895 MSE test 1.0753087990480266\n",
      "Epoch 5 / 10 loss: 222.35980600118637\n",
      "MSE train 1.0043999685039489 MSE test 1.0656668870258745\n",
      "Epoch 6 / 10 loss: 220.11240422725677\n",
      "MSE train 0.9944128235518362 MSE test 1.0562435045196878\n",
      "Epoch 7 / 10 loss: 217.91174709796906\n",
      "MSE train 0.9846290206820911 MSE test 1.0470298601527908\n",
      "Epoch 8 / 10 loss: 215.7544087767601\n",
      "MSE train 0.9750353707846351 MSE test 1.0380211002354516\n",
      "Epoch 9 / 10 loss: 213.63898587226868\n",
      "5min 19s ± 1.19 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a user\n",
    "sample_user = X_test.iloc[99,:]\n",
    "#get the predicted ratings\n",
    "sample_user_pred = sess.run(output_layer, feed_dict={input_layer:[sample_user]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
